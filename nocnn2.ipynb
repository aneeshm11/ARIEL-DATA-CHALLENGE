{"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"sourceId":70367,"databundleVersionId":9188054,"sourceType":"competition"},{"sourceId":9498960,"sourceType":"datasetVersion","datasetId":5618537,"isSourceIdPinned":true}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os \nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom scipy.stats import skew\nfrom scipy.stats import kurtosis\nfrom scipy.stats import entropy","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2024-10-24T16:54:53.873262Z","iopub.execute_input":"2024-10-24T16:54:53.873794Z","iopub.status.idle":"2024-10-24T16:54:59.491240Z","shell.execute_reply.started":"2024-10-24T16:54:53.873743Z","shell.execute_reply":"2024-10-24T16:54:59.489746Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"def vector(data, start, end , start1 , end1):\n    \n    region1= data[:start]  \n    \n    region2=data[start:end]\n    region3=data[end:start1]\n    region4=data[start1:end1]\n\n    region5=data[end1:]\n\n    uncovered = (  np.mean(region1) + np.mean(region5)  )/2\n    \n    reduction1= ( uncovered - np.mean(region2)    ) / uncovered\n    reduction2= ( uncovered - np.mean(region3)   )  / uncovered\n    reduction3= ( uncovered - np.mean(region4)   )  / uncovered\n        \n    \n    slope=(data[end]-data[start])/(end-start)\n    slope1=(data[end1]-data[start1])/(end1-start1)\n    \n    nr = np.mean(data)    / np.std(data)\n    nr1= np.mean(region1) / np.std(region1)\n    nr2= np.mean(region2) / np.std(region2)\n    nr3= np.mean(region3) / np.std(region3)\n    nr4= np.mean(region4) / np.std(region4)\n    nr5= np.mean(region5) / np.std(region5)\n   \n    skewness= skew(data)\n\n    input_vector = np.array([ slope, slope1 , reduction1, reduction2, reduction3 ,nr,nr1,nr2,nr3,nr4,nr5 ,skewness] )\n\n    return input_vector\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T16:54:59.493941Z","iopub.execute_input":"2024-10-24T16:54:59.498458Z","iopub.status.idle":"2024-10-24T16:54:59.514358Z","shell.execute_reply.started":"2024-10-24T16:54:59.498395Z","shell.execute_reply":"2024-10-24T16:54:59.512611Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from scipy.signal import savgol_filter\n\ndef smooth_data(data, window_size):\n    return savgol_filter(data, window_size, 3)\n\ndef optimize_breakpoint(data, initial_breakpoint, window_size, buffer_size, smooth_window):\n    best_breakpoint = initial_breakpoint\n    best_score = float(\"-inf\")\n    midpoint = len(data) // 2\n    smoothed_data = smooth_data(data, smooth_window)\n#     smoothed_data=data\n    for i in range(-window_size, window_size):\n        new_breakpoint = initial_breakpoint + i\n        if new_breakpoint > buffer_size and new_breakpoint < midpoint - buffer_size:\n            region1 = data[: new_breakpoint - buffer_size]\n            region2 = data[\n                new_breakpoint\n                + buffer_size : 2 * midpoint\n                - new_breakpoint\n                - buffer_size\n            ]\n            region3 = data[2 * midpoint - new_breakpoint + buffer_size :]\n\n            breakpoint_region1 = smoothed_data[new_breakpoint - buffer_size: new_breakpoint + buffer_size]\n            breakpoint_region2 = smoothed_data[new_breakpoint - buffer_size: new_breakpoint + buffer_size]\n\n            mean_diff = abs(np.mean(region1) - np.mean(region2)) + abs(\n                np.mean(region2) - np.mean(region3)\n            )\n            var_sum = np.var(region1) + np.var(region2) + np.var(region3)\n            range_at_breakpoint1 = (np.max(breakpoint_region1) - np.min(breakpoint_region1))\n            range_at_breakpoint2 = (np.max(breakpoint_region2) - np.min(breakpoint_region2))\n\n            mean_range_at_breakpoint = (range_at_breakpoint1 + range_at_breakpoint2) / 2\n\n            score = mean_diff - 0.5 * var_sum + mean_range_at_breakpoint\n\n            if score > best_score:\n                best_score = score\n                best_breakpoint = new_breakpoint\n\n                \n    return best_breakpoint","metadata":{"execution":{"iopub.status.busy":"2024-10-24T16:54:59.516403Z","iopub.execute_input":"2024-10-24T16:54:59.517040Z","iopub.status.idle":"2024-10-24T16:54:59.604846Z","shell.execute_reply.started":"2024-10-24T16:54:59.516978Z","shell.execute_reply":"2024-10-24T16:54:59.603440Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# import torch\n# import torch.nn as nn\n\n# class WavelengthPredictor(nn.Module):\n#     def __init__(self):\n#         super(WavelengthPredictor, self).__init__()\n#         self.model = nn.Sequential(\n#             # Initial expansion from 24 to wider dimensions\n#             nn.Linear(24, 64),\n#             nn.BatchNorm1d(64),\n#             nn.ReLU(),\n#             nn.Linear(64, 128),\n            \n#             # First dense block\n#             nn.Linear(128, 256),\n#             nn.BatchNorm1d(256),\n#             nn.ReLU(),\n#             nn.Linear(256, 512),\n#             nn.BatchNorm1d(512),\n#             nn.ReLU(),\n            \n#             # Second dense block with residual-like double linear\n#             nn.Linear(512, 512),\n#             nn.Linear(512, 512),\n#             nn.BatchNorm1d(512),\n#             nn.ReLU(),\n            \n#             # Third dense block\n#             nn.Linear(512, 768),\n#             nn.BatchNorm1d(768),\n#             nn.ReLU(),\n#             nn.Linear(768, 512),\n#             nn.BatchNorm1d(512),\n#             nn.ReLU(),\n            \n#             # Fourth dense block\n#             nn.Linear(512, 384),\n#             nn.BatchNorm1d(384),\n#             nn.ReLU(),\n#             nn.Linear(384, 384),\n#             nn.BatchNorm1d(384),\n#             nn.ReLU(),\n            \n#             # Final contraction to target dimension\n#             nn.Linear(384, 283)\n#         )\n        \n#         # Initialize weights for better gradient flow\n#         self._initialize_weights()\n    \n#     def _initialize_weights(self):\n#         for m in self.modules():\n#             if isinstance(m, nn.Linear):\n#                 nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n#                 if m.bias is not None:\n#                     nn.init.constant_(m.bias, 0)\n#             elif isinstance(m, nn.BatchNorm1d):\n#                 nn.init.constant_(m.weight, 1)\n#                 nn.init.constant_(m.bias, 0)\n    \n#     def forward(self, x):\n#         return self.model(x)\n    ","metadata":{"execution":{"iopub.status.busy":"2024-10-24T16:54:59.608398Z","iopub.execute_input":"2024-10-24T16:54:59.609242Z","iopub.status.idle":"2024-10-24T16:54:59.618172Z","shell.execute_reply.started":"2024-10-24T16:54:59.609176Z","shell.execute_reply":"2024-10-24T16:54:59.616282Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class WavelengthPredictor(nn.Module):\n    def __init__(self, dropout_rate=0.2):\n        super(WavelengthPredictor, self).__init__()\n        self.model = nn.Sequential(\n            # Initial layer with gradual size increase\n            nn.Linear(24, 48),\n            nn.BatchNorm1d(48),\n            nn.ReLU(),\n            nn.Dropout(dropout_rate),\n            \n            # Hidden layer 1\n            nn.Linear(48, 96),\n            nn.BatchNorm1d(96),\n            nn.ReLU(),\n            nn.Dropout(dropout_rate),\n            \n            # Hidden layer 2\n            nn.Linear(96, 192),\n            nn.BatchNorm1d(192),\n            nn.ReLU(),\n            nn.Dropout(dropout_rate),\n            \n            # Hidden layer 3\n            nn.Linear(192, 256),\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n            nn.Dropout(dropout_rate),\n            \n            # Final layer with more gradual reduction\n            nn.Linear(256, 283)\n        )\n        \n        self._initialize_weights()\n    \n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                # Using Xavier/Glorot initialization for better gradient flow\n                nn.init.xavier_normal_(m.weight)\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm1d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n    \n    def forward(self, x):\n        return self.model(x)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T16:54:59.620299Z","iopub.execute_input":"2024-10-24T16:54:59.620881Z","iopub.status.idle":"2024-10-24T16:54:59.637581Z","shell.execute_reply.started":"2024-10-24T16:54:59.620813Z","shell.execute_reply":"2024-10-24T16:54:59.635946Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def planetnumber(filename):\n    return int(filename.split('_')[0])\n\nclass ARIEL(Dataset):\n    def __init__(self, airs_dir, fgs_dir , start , end ):\n        self.airs_dir = airs_dir\n        self.fgs_dir = fgs_dir\n        \n        self.airs_list = sorted(os.listdir(self.airs_dir), key=planetnumber)[start:end]\n        self.fgs_list = sorted(os.listdir(self.fgs_dir), key=planetnumber)[start:end]\n    \n    def __getitem__(self, index):\n        \n        airs_file = os.path.join(self.airs_dir, self.airs_list[index])\n        fgs_file = os.path.join(self.fgs_dir, self.fgs_list[index])\n        \n        planet =  planetnumber(self.airs_list[index])\n        \n        airs_arr = np.load(airs_file)\n        fgs_arr  = np.load(fgs_file) \n        \n        airs_arr=(airs_arr-np.min(airs_arr))/(np.max(airs_arr)-np.min(airs_arr))\n        fgs_arr=(fgs_arr-np.min(fgs_arr))/(np.max(fgs_arr)-np.min(fgs_arr))\n        \n        initial_breakpoint=900\n        buffer_size=80 \n        smooth_window=200\n        window_size=300\n\n        airsbp = optimize_breakpoint(airs_arr,initial_breakpoint,window_size=window_size,buffer_size=buffer_size,smooth_window=smooth_window)\n        fgsbp = optimize_breakpoint(fgs_arr,initial_breakpoint,window_size=window_size,buffer_size=buffer_size,smooth_window=250)\n\n        midpoint1 = len(airs_arr) // 2\n        bp1 = [airsbp, 2 * midpoint1 - airsbp]\n        airs_start   =  bp1[0] - buffer_size\n        airs_end     =  bp1[0] + buffer_size\n        airs_start1  =  bp1[1] - buffer_size\n        airs_end1    =  bp1[1] + buffer_size\n        \n        midpoint2 = len(fgs_arr) // 2\n        bp2 = [fgsbp, 2 * midpoint2 - fgsbp]\n        fgs_start  =    bp2[0] - buffer_size\n        fgs_end    =    bp2[0] + buffer_size\n        fgs_start1 =    bp2[1] - buffer_size\n        fgs_end1   =    bp2[1] + buffer_size\n\n\n        airs_vector=  vector( airs_arr,  airs_start ,  airs_end , airs_start1 , airs_end1 )\n        fgs_vector =  vector( fgs_arr, fgs_start  ,   fgs_end , fgs_start1  , fgs_end1 )        \n        \n        \n        input_vector=  np.concatenate((airs_vector , fgs_vector))\n\n        \n        labels        = pd.read_csv(\"/kaggle/input/ariel-data-challenge-2024/train_labels.csv\")\n        filtered_data = labels[labels[\"planet_id\"] == planet].iloc[0, 1:].values\n        \n        \n        input_vector = torch.tensor(np.array(input_vector))\n\n        output       = torch.tensor(filtered_data )\n\n        \n        return input_vector , output  , planet\n     \n    def __len__(self):\n        return len(self.airs_list)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T16:54:59.639512Z","iopub.execute_input":"2024-10-24T16:54:59.640085Z","iopub.status.idle":"2024-10-24T16:54:59.664783Z","shell.execute_reply.started":"2024-10-24T16:54:59.640024Z","shell.execute_reply":"2024-10-24T16:54:59.662788Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)\n\nmodel = WavelengthPredictor()\nmodel = nn.DataParallel(model)\nmodel = model.to(device)\n\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-2 , weight_decay=1e-4 )\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer,  factor=0.5, patience=3 , min_lr=1e-8)\n\n# weights=\"/kaggle/input/arieldata/epoch140.pth\"\nweights=None\n\nif weights:\n    checkpoint = torch.load(weights, map_location=device)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n    start_epoch = checkpoint['epoch'] + 1\n    print(f\"Resuming from epoch {start_epoch}\")\n    print(f\"Resuming learning rate: {optimizer.param_groups[0]['lr']}\")\n\n    \nelse:\n    start_epoch = 0\n    \nbatchsize1=1\nbatchsize2=2\n\ntrain_data = ARIEL(\"/kaggle/input/arieldata/airs2k\", \"/kaggle/input/arieldata/fgs2k\" , start=0 , end=673)\ntrain_dataloader = DataLoader(train_data, batch_size=batchsize1, shuffle=False , num_workers=8)\n\nval_data = ARIEL(\"/kaggle/input/arieldata/airs2k\", \"/kaggle/input/arieldata/fgs2k\" , start=612 , end=662)\nval_dataloader = DataLoader(val_data, batch_size=batchsize2, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T17:01:58.013805Z","iopub.execute_input":"2024-10-24T17:01:58.015162Z","iopub.status.idle":"2024-10-24T17:01:58.049099Z","shell.execute_reply.started":"2024-10-24T17:01:58.015110Z","shell.execute_reply":"2024-10-24T17:01:58.047452Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"cpu\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"}]},{"cell_type":"code","source":"l=[]\nfor a,b,c in train_dataloader:\n    l.append(np.array(a)[0])\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T17:01:59.132738Z","iopub.execute_input":"2024-10-24T17:01:59.133189Z","iopub.status.idle":"2024-10-24T17:03:36.007239Z","shell.execute_reply.started":"2024-10-24T17:01:59.133148Z","shell.execute_reply":"2024-10-24T17:03:36.005456Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"np.save(\"a24.npy\" , np.array(l))","metadata":{"execution":{"iopub.status.busy":"2024-10-24T17:03:36.010038Z","iopub.execute_input":"2024-10-24T17:03:36.010527Z","iopub.status.idle":"2024-10-24T17:03:36.021209Z","shell.execute_reply.started":"2024-10-24T17:03:36.010478Z","shell.execute_reply":"2024-10-24T17:03:36.019945Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"print(np.array(l)[1][:5])","metadata":{"execution":{"iopub.status.busy":"2024-10-24T17:04:21.767637Z","iopub.execute_input":"2024-10-24T17:04:21.768261Z","iopub.status.idle":"2024-10-24T17:04:21.781205Z","shell.execute_reply.started":"2024-10-24T17:04:21.768211Z","shell.execute_reply":"2024-10-24T17:04:21.779267Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"[-0.00386623  0.00299635  0.42455465  0.73300601  0.27937264]\n","output_type":"stream"}]},{"cell_type":"code","source":"print(len(train_dataloader))","metadata":{"execution":{"iopub.status.busy":"2024-10-22T11:24:54.104859Z","iopub.execute_input":"2024-10-22T11:24:54.105702Z","iopub.status.idle":"2024-10-22T11:24:54.110367Z","shell.execute_reply.started":"2024-10-22T11:24:54.105654Z","shell.execute_reply":"2024-10-22T11:24:54.109492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"only airs , features = 24\")","metadata":{"execution":{"iopub.status.busy":"2024-10-22T11:24:54.111599Z","iopub.execute_input":"2024-10-22T11:24:54.112183Z","iopub.status.idle":"2024-10-22T11:24:54.119492Z","shell.execute_reply.started":"2024-10-22T11:24:54.112137Z","shell.execute_reply":"2024-10-22T11:24:54.118575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_params = sum(p.numel() for p in model.parameters())\nprint(f\"Number of parameters: {num_params}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-22T11:24:54.120588Z","iopub.execute_input":"2024-10-22T11:24:54.121218Z","iopub.status.idle":"2024-10-22T11:24:54.129937Z","shell.execute_reply.started":"2024-10-22T11:24:54.121165Z","shell.execute_reply":"2024-10-22T11:24:54.129094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)","metadata":{"execution":{"iopub.status.busy":"2024-10-22T11:24:54.131154Z","iopub.execute_input":"2024-10-22T11:24:54.131546Z","iopub.status.idle":"2024-10-22T11:24:54.138344Z","shell.execute_reply.started":"2024-10-22T11:24:54.131487Z","shell.execute_reply":"2024-10-22T11:24:54.137469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 101\ntotal = start_epoch + epochs\n\nprint(\"training started\")\n\nbest_val_loss = float('inf')\npatience = 15\nno_improve = 0\n\nfor epoch in range(start_epoch, total):\n    model.train()\n    train_loss = 0\n    val_loss = 0\n    \n    for input_vector, label , planet in train_dataloader:\n        optimizer.zero_grad()\n        input_vector = input_vector.float().to(device)\n        label = label.float().to(device)\n        \n        out = model(input_vector)\n        loss = criterion(out, label)\n        \n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item()\n    \n    train_loss /= len(train_dataloader)\n    \n    if epoch%5==0 and epoch>0:\n        print(f\" label {(label[0][:3].cpu().detach().numpy())} , output {(out[0][:3].cpu().detach().numpy())}\")\n\n    if epoch==13:\n        for param_group in optimizer.param_groups:\n            param_group['lr'] = 1e-8\n            print(f\"Learning rate manually set to 1e-4 at epoch {epoch}\")\n\n    model.eval()\n    with torch.no_grad():\n        for input_vector, label , planet in val_dataloader:\n            input_vector = input_vector.float().to(device)\n            label = label.float().to(device)\n            \n            out = model(input_vector)\n            loss = criterion(out, label)\n            val_loss += loss.item()\n    \n    val_loss /= len(val_dataloader)\n    \n    prev = optimizer.param_groups[0]['lr']\n    scheduler.step(val_loss)\n    nex =optimizer.param_groups[0]['lr']\n    \n    if prev!=nex:\n        print(\"LR decreased to \" , nex)\n    \n    print(f\"Epoch {epoch+1}/{total}, Train loss: {train_loss}, Val loss: {val_loss}\")\n    \n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        no_improve = 0\n        \n        model_filename = f\"epoch{epoch}-loss{train_loss:.8f}.pth\"\n        model_path = os.path.join(\"/kaggle/working\", model_filename)\n        \n        checkpoint = {\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'scheduler_state_dict': scheduler.state_dict(),\n            'loss': train_loss,\n        }\n        torch.save(checkpoint, model_path)\n        print(f\"Model saved at epoch {epoch}\")\n        \n    else:\n        no_improve += 1\n        if no_improve == patience:\n            print(\"Early stopping triggered at epoch\", epoch)\n            break","metadata":{"execution":{"iopub.status.busy":"2024-10-22T11:24:54.139575Z","iopub.execute_input":"2024-10-22T11:24:54.139875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Current learning rate: {optimizer.param_groups[0]['lr']}\")\nprint(\"done\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# planet=1129361124\n# airs_file=f\"/kaggle/input/arieldata/airs2k/{planet}_airs.npy\"\n# # fgs_file=f\"/kaggle/input/dataset-ariel/fgs/{planet}_fgs.npy\"\n\n\n# airs_arr = np.load(airs_file)\n# # fgs_arr  = np.load(fgs_file) \n\n# airs_arr=(airs_arr-np.min(airs_arr))/(np.max(airs_arr)-np.min(airs_arr))\n# # fgs_arr=(fgs_arr-np.min(fgs_arr))/(np.max(fgs_arr)-np.min(fgs_arr))\n# initial_breakpoint=900\n# buffer_size=80 \n# smooth_window=200\n# window_size=300\n# airsbp = optimize_breakpoint(airs_arr,initial_breakpoint,window_size=window_size,buffer_size=buffer_size,smooth_window=smooth_window)\n# # fgsbp = optimize_breakpoint(fgs_arr,initial_breakpoint,window_size=window_size,buffer_size=buffer_size,smooth_window=smooth_window)\n# midpoint1 = len(airs_arr) // 2\n# bp1 = [airsbp, 2 * midpoint1 - airsbp]\n# airs_start   =  bp1[0] - buffer_size\n# airs_end     =  bp1[0] + buffer_size\n# airs_start1  =  bp1[1] - buffer_size\n# airs_end1    =  bp1[1] + buffer_size\n# # midpoint2 = len(fgs_arr) // 2\n# # bp2 = [fgsbp, 2 * midpoint2 - fgsbp]\n# # fgs_start  =    bp2[0] - buffer_size\n# # fgs_end    =    bp2[0] + buffer_size\n# # fgs_start1 =    bp2[1] - buffer_size\n# # fgs_end1   =    bp2[1] + buffer_size\n# airs_vector=  vector( airs_arr,  airs_start ,  airs_end , airs_start1 , airs_end1 )\n# # fgs_vector =  vector( fgs_arr, fgs_start  ,   fgs_end , fgs_start1  , fgs_end1 )        \n\n\n# # input_vector=  np.concatenate((airs_vector , fgs_vector))\n# # input_vector=airs_vector\n\n# labels        = pd.read_csv(\"/kaggle/input/ariel-data-challenge-2024/train_labels.csv\")\n# filtered_data = labels[labels[\"planet_id\"] == planet].iloc[0, 1:].values\n# in_vector = torch.tensor(np.array(airs_vector))\n# in_vector=in_vector.unsqueeze(0).float()\n# output       = torch.tensor(filtered_data )\n# print(in_vector.shape , output.shape)\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(output[:5])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# model = WavelengthPredictor()\n# model = nn.DataParallel(model)\n# model = model.to(device)\n\n# weights=\"/kaggle/input/arieldata/ariel3_335.pth\"\n\n# checkpoint = torch.load(weights, map_location=device)\n# model.load_state_dict(checkpoint['model_state_dict'])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model.eval()\n# with torch.no_grad():\n#     pred = model(in_vector)\n\n# # Print the output shape\n# print(pred.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(pred[0][:5])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
