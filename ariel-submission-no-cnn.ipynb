{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":70367,"databundleVersionId":9188054,"sourceType":"competition"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os \nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom scipy.stats import skew\nfrom scipy.stats import kurtosis\nfrom scipy.stats import entropy\nimport torch.nn.functional as F\nfrom scipy.signal import savgol_filter","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def process_signal(signal, gain, offset, dead, flat, dark, linear_corr):\n    signal = signal * gain + offset\n    flat = np.ma.masked_where(dead, flat)\n    dark = np.ma.masked_where(dead, dark)\n    flat = np.tile(flat, (signal.shape[0], 1, 1))\n    dark = np.tile(dark, (signal.shape[0], 1, 1))\n    dead = np.tile(dead, (signal.shape[0], 1, 1))\n    \n    signal = np.ma.masked_where(dead, signal)\n    signal = (signal - dark) / (flat - dark)\n#     linear_corr = np.flip(linear_corr, axis=0)\n#     for x in range(signal.shape[1]):\n#         for y in range(signal.shape[2]):\n#             poli = np.poly1d(linear_corr[:, x, y])\n#             signal[:, x, y] = poli(signal[:, x, y])\n    signal = signal[1::2, :, :] - signal[::2, :, :]    \n    return signal\n\ndef bin_obs(cds_signal, binning):\n    cds_transposed = cds_signal.transpose(1, 2, 0)\n    binned_shape = (cds_transposed.shape[0], cds_transposed.shape[1], cds_transposed.shape[2] // binning)\n    cds_binned = np.zeros(binned_shape)\n    for i in range(binned_shape[2]):\n        cds_binned[:, :, i] = np.sum(cds_transposed[:, :, i*binning:(i+1)*binning], axis=2)\n    return cds_binned.transpose(2, 0, 1)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def smooth_data(data, window_size):\n    return savgol_filter(data, window_size, 3)\n\ndef optimize_breakpoint(data, initial_breakpoint, window_size, buffer_size, smooth_window):\n    best_breakpoint = initial_breakpoint\n    best_score = float(\"-inf\")\n    midpoint = len(data) // 2\n    smoothed_data = smooth_data(data, smooth_window)\n#     smoothed_data=data\n    for i in range(-window_size, window_size):\n        new_breakpoint = initial_breakpoint + i\n        if new_breakpoint > buffer_size and new_breakpoint < midpoint - buffer_size:\n            region1 = data[: new_breakpoint - buffer_size]\n            region2 = data[\n                new_breakpoint\n                + buffer_size : 2 * midpoint\n                - new_breakpoint\n                - buffer_size\n            ]\n            region3 = data[2 * midpoint - new_breakpoint + buffer_size :]\n\n            breakpoint_region1 = smoothed_data[new_breakpoint - buffer_size: new_breakpoint + buffer_size]\n            breakpoint_region2 = smoothed_data[new_breakpoint - buffer_size: new_breakpoint + buffer_size]\n\n            mean_diff = abs(np.mean(region1) - np.mean(region2)) + abs(\n                np.mean(region2) - np.mean(region3)\n            )\n            var_sum = np.var(region1) + np.var(region2) + np.var(region3)\n            range_at_breakpoint1 = (np.max(breakpoint_region1) - np.min(breakpoint_region1))\n            range_at_breakpoint2 = (np.max(breakpoint_region2) - np.min(breakpoint_region2))\n\n            mean_range_at_breakpoint = (range_at_breakpoint1 + range_at_breakpoint2) / 2\n\n            score = mean_diff - 0.5 * var_sum + mean_range_at_breakpoint\n\n            if score > best_score:\n                best_score = score\n                best_breakpoint = new_breakpoint\n\n                \n    return best_breakpoint","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"        \ndef vector(data, start, end , start1 , end1):\n    \n    region1= data[:start]  \n    \n    region2=data[start:end]\n    region3=data[end:start1]\n    region4=data[start1:end1]\n\n    region5=data[end1:]\n\n    uncovered = (  np.mean(region1) + np.mean(region5)  )/2\n    \n    reduction1= ( uncovered - np.mean(region2)    ) / uncovered\n    reduction2= ( uncovered - np.mean(region3)   )  / uncovered\n    reduction3= ( uncovered - np.mean(region4)   )  / uncovered\n        \n    \n    slope=(data[end]-data[start])/(end-start)\n    slope1=(data[end1]-data[start1])/(end1-start1)\n    \n    nr = np.mean(data)    / np.std(data)\n    nr1= np.mean(region1) / np.std(region1)\n    nr2= np.mean(region2) / np.std(region2)\n    nr3= np.mean(region3) / np.std(region3)\n    nr4= np.mean(region4) / np.std(region4)\n    nr5= np.mean(region5) / np.std(region5)\n   \n    skewness= skew(data)\n\n    input_vector = np.array([ slope, slope1 , reduction1, reduction2, reduction3 ,nr,nr1,nr2,nr3,nr4,nr5 ,skewness] )\n\n    return input_vector\n\n\ndef final_vector(airs_arr , fgs_arr):\n\n    airs_arr=(airs_arr-np.min(airs_arr))/(np.max(airs_arr)-np.min(airs_arr))\n    fgs_arr=(fgs_arr-np.min(fgs_arr))/(np.max(fgs_arr)-np.min(fgs_arr))\n    \n    initial_breakpoint=850\n    buffer_size=80 \n    smooth_window=200\n    window_size=300\n    airsbp = optimize_breakpoint(airs_arr,initial_breakpoint,window_size=window_size,buffer_size=buffer_size,smooth_window=smooth_window)\n    fgsbp = optimize_breakpoint(fgs_arr,initial_breakpoint,window_size=window_size,buffer_size=buffer_size,smooth_window=250)\n    midpoint1 = len(airs_arr) // 2\n    bp1 = [airsbp, 2 * midpoint1 - airsbp]\n    airs_start   =  bp1[0] - buffer_size\n    airs_end     =  bp1[0] + buffer_size\n    airs_start1  =  bp1[1] - buffer_size\n    airs_end1    =  bp1[1] + buffer_size\n    \n    midpoint2 = len(fgs_arr) // 2\n    bp2 = [fgsbp, 2 * midpoint2 - fgsbp]\n    fgs_start  =    bp2[0] - buffer_size\n    fgs_end    =    bp2[0] + buffer_size\n    fgs_start1 =    bp2[1] - buffer_size\n    fgs_end1   =    bp2[1] + buffer_size\n    airs_vector=  vector( airs_arr,  airs_start ,  airs_end , airs_start1 , airs_end1 )\n    fgs_vector =  vector( fgs_arr, fgs_start  ,   fgs_end , fgs_start1  , fgs_end1 )        \n    \n    f_vector=  np.concatenate((airs_vector , fgs_vector))\n    \n    return f_vector ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def planetnumber(filename):\n    return int(filename.split('_')[0])\n\nclass PlanetDataset(Dataset):\n    def __init__(self, mode):\n        self.mode = mode\n        self.adc_info = pd.read_csv(f'/kaggle/input/ariel-data-challenge-2024/{mode}_adc_info.csv')\n        self.files = sorted(os.listdir(f\"/kaggle/input/ariel-data-challenge-2024/{mode}\"), key=planetnumber)\n            \n    def __len__(self):\n        return len(self.files)\n    \n    def __getitem__(self, idx):\n        planet = int(self.files[idx])\n        selected_adc_info = self.adc_info.loc[self.adc_info['planet_id'] == planet]\n        \n        airs_dark = pd.read_parquet(f'/kaggle/input/ariel-data-challenge-2024/{self.mode}/{planet}/AIRS-CH0_calibration/dark.parquet').values.reshape(32, 356)\n        airs_dead = pd.read_parquet(f'/kaggle/input/ariel-data-challenge-2024/{self.mode}/{planet}/AIRS-CH0_calibration/dead.parquet').values.reshape(32, 356)\n        airs_flat = pd.read_parquet(f'/kaggle/input/ariel-data-challenge-2024/{self.mode}/{planet}/AIRS-CH0_calibration/flat.parquet').values.reshape(32, 356)\n        airs_linear_corr = pd.read_parquet(f'/kaggle/input/ariel-data-challenge-2024/{self.mode}/{planet}/AIRS-CH0_calibration/linear_corr.parquet').values.reshape(6, 32, 356)\n        airs_ch0 = pd.read_parquet(f'/kaggle/input/ariel-data-challenge-2024/{self.mode}/{planet}/AIRS-CH0_signal.parquet').values\n        airs_ch0_gain = selected_adc_info['AIRS-CH0_adc_gain'].values[0]\n        airs_ch0_offset = selected_adc_info['AIRS-CH0_adc_offset'].values[0]\n        airs_ch0 = airs_ch0.reshape(11250, 32, 356)\n        \n        # Load and process FGS data\n        fgs_dark = pd.read_parquet(f'/kaggle/input/ariel-data-challenge-2024/{self.mode}/{planet}/FGS1_calibration/dark.parquet').values.reshape(32, 32)\n        fgs_dead = pd.read_parquet(f'/kaggle/input/ariel-data-challenge-2024/{self.mode}/{planet}/FGS1_calibration/dead.parquet').values.reshape(32, 32)\n        fgs_flat = pd.read_parquet(f'/kaggle/input/ariel-data-challenge-2024/{self.mode}/{planet}/FGS1_calibration/flat.parquet').values.reshape(32, 32)\n        fgs_linear_corr = pd.read_parquet(f'/kaggle/input/ariel-data-challenge-2024/{self.mode}/{planet}/FGS1_calibration/linear_corr.parquet').values.reshape(6, 32, 32)\n        fgs1 = pd.read_parquet(f'/kaggle/input/ariel-data-challenge-2024/{self.mode}/{planet}/FGS1_signal.parquet').values\n        fgs1_gain = selected_adc_info['FGS1_adc_gain'].values[0]\n        fgs1_offset = selected_adc_info['FGS1_adc_offset'].values[0]\n        fgs1 = fgs1.reshape(135000, 32, 32)\n        \n        airs_ch0_processed = process_signal(airs_ch0, airs_ch0_gain, airs_ch0_offset, airs_dead, airs_flat, airs_dark, airs_linear_corr)\n        fgs1_processed = process_signal(fgs1, fgs1_gain, fgs1_offset, fgs_dead, fgs_flat, fgs_dark, fgs_linear_corr)\n        \n        # Bin observations\n        airs_frames = bin_obs(airs_ch0_processed, binning=2)\n        fgs_frames = bin_obs(fgs1_processed, binning=25)\n        \n        # Convert to PyTorch tensors and normalize\n        airs_1d = torch.tensor(np.sum(airs_frames, axis=(1,2))).float()\n        fgs_1d = torch.tensor(np.sum(fgs_frames, axis=(1,2))).float()\n        \n        airs_1d = (airs_1d - airs_1d.min()) / (airs_1d.max() - airs_1d.min())\n        fgs_1d = (fgs_1d - fgs_1d.min()) / (fgs_1d.max() - fgs_1d.min())\n        \n        airs_1d = airs_1d\n        fgs_1d = fgs_1d\n    \n        input_vector = final_vector( airs_1d , fgs_1d)\n        \n        return input_vector , planet","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device1 = torch.device('cuda')\n\n\nmodel1 = Model(airs_frames, fgs_frames).to(device1)\nmodel1 = nn.DataParallel(model1).to(device1)\n\nweights1=\"/kaggle/input/arieldata/main_model_weights.pth\"\n\n\ncheckpoint1 = torch.load(weights1, map_location=device1 , weights_only=True)\nmodel1.load_state_dict(checkpoint1['model_state_dict'])\n\n\n\ncuda_count = torch.cuda.device_count()\nprint(f\"Number of CUDA devices: {cuda_count}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = PlanetDataset(mode=\"test\")\nbatchsize=1\nt=1\ndataloader = DataLoader(dataset, batch_size=batchsize, shuffle=False , num_workers=2)\n\nmodel1.eval()\nfinal = []\ni=0\nvals=[]\n\nwith torch.no_grad():\n    for vec , planet in dataloader:\n        vals.append(planet.item())\n        output         = model1(vec)\n        outs  = np.abs(output[0])\n        final.append(outs)\n            \nprint(\"done\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}