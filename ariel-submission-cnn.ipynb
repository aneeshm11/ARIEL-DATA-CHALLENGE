{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":70367,"databundleVersionId":9188054,"sourceType":"competition"},{"sourceId":9704794,"sourceType":"datasetVersion","datasetId":5618537}],"dockerImageVersionId":30787,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"papermill":{"default_parameters":{},"duration":30.820989,"end_time":"2024-10-15T14:05:06.245961","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-10-15T14:04:35.424972","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os \nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom scipy.stats import skew\nfrom scipy.stats import kurtosis\nfrom scipy.stats import entropy\nimport torch.nn.functional as F\nfrom scipy.signal import savgol_filter\nfrom statsmodels.nonparametric.smoothers_lowess import lowess","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":5.743759,"end_time":"2024-10-15T14:04:44.139431","exception":false,"start_time":"2024-10-15T14:04:38.395672","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-23T18:40:13.136347Z","iopub.execute_input":"2024-10-23T18:40:13.137024Z","iopub.status.idle":"2024-10-23T18:40:17.553002Z","shell.execute_reply.started":"2024-10-23T18:40:13.136978Z","shell.execute_reply":"2024-10-23T18:40:17.552227Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"def process_signal(signal, gain, offset, dead, flat, dark, linear_corr):\n    signal = signal * gain + offset\n    flat = np.ma.masked_where(dead, flat)\n    dark = np.ma.masked_where(dead, dark)\n    flat = np.tile(flat, (signal.shape[0], 1, 1))\n    dark = np.tile(dark, (signal.shape[0], 1, 1))\n    dead = np.tile(dead, (signal.shape[0], 1, 1))\n    \n    signal = np.ma.masked_where(dead, signal)\n    signal = (signal - dark) / (flat - dark)\n#     linear_corr = np.flip(linear_corr, axis=0)\n#     for x in range(signal.shape[1]):\n#         for y in range(signal.shape[2]):\n#             poli = np.poly1d(linear_corr[:, x, y])\n#             signal[:, x, y] = poli(signal[:, x, y])\n    signal = signal[1::2, :, :] - signal[::2, :, :]    \n    return signal\n\ndef bin_obs(cds_signal, binning):\n    cds_transposed = cds_signal.transpose(1, 2, 0)\n    binned_shape = (cds_transposed.shape[0], cds_transposed.shape[1], cds_transposed.shape[2] // binning)\n    cds_binned = np.zeros(binned_shape)\n    for i in range(binned_shape[2]):\n        cds_binned[:, :, i] = np.sum(cds_transposed[:, :, i*binning:(i+1)*binning], axis=2)\n    return cds_binned.transpose(2, 0, 1)\n","metadata":{"papermill":{"duration":0.015749,"end_time":"2024-10-15T14:04:44.174377","exception":false,"start_time":"2024-10-15T14:04:44.158628","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-23T18:40:17.554521Z","iopub.execute_input":"2024-10-23T18:40:17.554927Z","iopub.status.idle":"2024-10-23T18:40:17.564393Z","shell.execute_reply.started":"2024-10-23T18:40:17.554894Z","shell.execute_reply":"2024-10-23T18:40:17.563482Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def smooth_data(data, window_size):\n    return savgol_filter(data, window_size, 3)\n\ndef optimize_breakpoint(data, initial_breakpoint, window_size, buffer_size, smooth_window):\n    best_breakpoint = initial_breakpoint\n    best_score = float(\"-inf\")\n    midpoint = len(data) // 2\n    smoothed_data = smooth_data(data, smooth_window)\n#     smoothed_data=data\n    for i in range(-window_size, window_size):\n        new_breakpoint = initial_breakpoint + i\n        if new_breakpoint > buffer_size and new_breakpoint < midpoint - buffer_size:\n            region1 = data[: new_breakpoint - buffer_size]\n            region2 = data[\n                new_breakpoint\n                + buffer_size : 2 * midpoint\n                - new_breakpoint\n                - buffer_size\n            ]\n            region3 = data[2 * midpoint - new_breakpoint + buffer_size :]\n\n            breakpoint_region1 = smoothed_data[new_breakpoint - buffer_size: new_breakpoint + buffer_size]\n            breakpoint_region2 = smoothed_data[new_breakpoint - buffer_size: new_breakpoint + buffer_size]\n\n            mean_diff = abs(np.mean(region1) - np.mean(region2)) + abs(\n                np.mean(region2) - np.mean(region3)\n            )\n            var_sum = np.var(region1) + np.var(region2) + np.var(region3)\n            range_at_breakpoint1 = (np.max(breakpoint_region1) - np.min(breakpoint_region1))\n            range_at_breakpoint2 = (np.max(breakpoint_region2) - np.min(breakpoint_region2))\n\n            mean_range_at_breakpoint = (range_at_breakpoint1 + range_at_breakpoint2) / 2\n\n            score = mean_diff - 0.5 * var_sum + mean_range_at_breakpoint\n\n            if score > best_score:\n                best_score = score\n                best_breakpoint = new_breakpoint\n\n                \n    return best_breakpoint","metadata":{"execution":{"iopub.status.busy":"2024-10-23T18:40:17.565405Z","iopub.execute_input":"2024-10-23T18:40:17.565775Z","iopub.status.idle":"2024-10-23T18:40:17.576206Z","shell.execute_reply.started":"2024-10-23T18:40:17.565733Z","shell.execute_reply":"2024-10-23T18:40:17.575302Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class ResidualModel(nn.Module):\n    def __init__(self):\n        super(ResidualModel, self).__init__()\n        self.fc0 = nn.Linear(24, 48)\n        self.fc1 = nn.Linear(48, 64)\n        self.fc2 = nn.Linear(64, 128)\n        self.fc3 = nn.Linear(128, 256)\n        self.fc4 = nn.Linear(256, 283)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        x = self.relu(self.fc0(x))\n        x = self.relu(self.fc1(x))\n        x = self.relu(self.fc2(x))\n        x = self.relu(self.fc3(x))\n        x = self.fc4(x)\n        return x\n","metadata":{"execution":{"iopub.status.busy":"2024-10-23T18:40:17.577892Z","iopub.execute_input":"2024-10-23T18:40:17.578217Z","iopub.status.idle":"2024-10-23T18:40:17.589937Z","shell.execute_reply.started":"2024-10-23T18:40:17.578169Z","shell.execute_reply":"2024-10-23T18:40:17.589211Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class CNN2D_AIRS(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(CNN2D_AIRS, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n\n    def forward(self, x):\n#         x = self.pool(F.relu(self.bn1(self.conv1(x))))\n#         x = self.pool(F.relu(self.bn2(self.conv2(x))))\n\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n\n        return x\n\nclass CNN2D_FGS(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(CNN2D_FGS, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n\n    def forward(self, x):\n#         x = self.pool(F.relu(self.bn1(self.conv1(x))))\n#         x = self.pool(F.relu(self.bn2(self.conv2(x))))\n\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        return x\n\nclass Model(nn.Module):\n    def __init__(self, airs_frames, fgs_frames):\n        super(Model, self).__init__()\n        self.cnn_airs = CNN2D_AIRS(1, 16)\n        self.cnn_fgs = CNN2D_FGS(1, 16)\n        \n        self.lstm_airs = nn.LSTM(16 * 8 * 89, 128, batch_first=True)\n        self.lstm_fgs = nn.LSTM(16 * 8 * 8, 128, batch_first=True)\n        \n        self.bn_airs = nn.BatchNorm1d(128)\n        self.bn_fgs = nn.BatchNorm1d(128)\n        \n        self.fc_light_curve_airs = nn.Sequential(\n            nn.Linear(airs_frames, 64),\n            nn.ReLU()\n#             nn.BatchNorm1d(64),\n\n        )\n        \n        self.fc_light_curve_fgs = nn.Sequential(\n            nn.Linear(fgs_frames, 64),\n            nn.ReLU()\n#             nn.BatchNorm1d(64),\n\n        )\n        \n        self.fc_combined = nn.Sequential(            \n            nn.Linear(128 + 128 + 64 + 64, 256),\n            nn.ReLU(),\n            nn.BatchNorm1d(256),\n#             nn.Dropout(0.15),\n            nn.Linear(256, 256),\n            nn.ReLU(),\n            nn.BatchNorm1d(256),\n#             nn.Dropout(0.15),\n            nn.Linear(256, 283)\n        )\n\n    def forward(self, airs_ch0, fgs1, light_curve_airs, light_curve_fgs):\n        batch_size, frames, _, _, _ = airs_ch0.shape\n        \n        airs_features = self.cnn_airs(airs_ch0.view(-1, 1, 32, 356))\n        airs_features = airs_features.view(batch_size, frames, -1)\n        _, (airs_hidden, _) = self.lstm_airs(airs_features)\n#         airs_hidden = self.bn_airs(airs_hidden.squeeze(0))\n        airs_hidden = airs_hidden.squeeze(0)\n        \n        \n        fgs_features = self.cnn_fgs(fgs1.view(-1, 1, 32, 32))\n        fgs_features = fgs_features.view(batch_size, frames, -1)\n        _, (fgs_hidden, _) = self.lstm_fgs(fgs_features)\n#         fgs_hidden = self.bn_fgs(fgs_hidden.squeeze(0))\n        fgs_hidden = fgs_hidden.squeeze(0)\n        \n        light_curve_airs_features = self.fc_light_curve_airs(light_curve_airs)\n        light_curve_fgs_features = self.fc_light_curve_fgs(light_curve_fgs)\n        \n        combined_features = torch.cat((airs_hidden, fgs_hidden, light_curve_airs_features, light_curve_fgs_features), dim=1)\n        \n        output = self.fc_combined(combined_features)\n        return output\n","metadata":{"papermill":{"duration":0.110632,"end_time":"2024-10-15T14:04:44.288840","exception":false,"start_time":"2024-10-15T14:04:44.178208","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-23T18:40:17.591122Z","iopub.execute_input":"2024-10-23T18:40:17.591478Z","iopub.status.idle":"2024-10-23T18:40:17.612021Z","shell.execute_reply.started":"2024-10-23T18:40:17.591435Z","shell.execute_reply":"2024-10-23T18:40:17.611243Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"        \ndef vector(data, start, end , start1 , end1):\n    \n    region1= data[:start]  \n    \n    region2=data[start:end]\n    region3=data[end:start1]\n    region4=data[start1:end1]\n\n    region5=data[end1:]\n\n    uncovered = (  np.mean(region1) + np.mean(region5)  )/2\n    \n    reduction1= ( uncovered - np.mean(region2)    ) / uncovered\n    reduction2= ( uncovered - np.mean(region3)   )  / uncovered\n    reduction3= ( uncovered - np.mean(region4)   )  / uncovered\n        \n    \n    slope=(data[end]-data[start])/(end-start)\n    slope1=(data[end1]-data[start1])/(end1-start1)\n    \n    nr = np.mean(data)    / np.std(data)\n    nr1= np.mean(region1) / np.std(region1)\n    nr2= np.mean(region2) / np.std(region2)\n    nr3= np.mean(region3) / np.std(region3)\n    nr4= np.mean(region4) / np.std(region4)\n    nr5= np.mean(region5) / np.std(region5)\n   \n    skewness= skew(data)\n\n    input_vector = np.array([ slope, slope1 , reduction1, reduction2, reduction3 ,nr,nr1,nr2,nr3,nr4,nr5 ,skewness] )\n\n    return input_vector\n\n\ndef final_vector(airs_arr , fgs_arr):\n\n    airs_arr=(airs_arr-np.min(airs_arr))/(np.max(airs_arr)-np.min(airs_arr))\n    fgs_arr=(fgs_arr-np.min(fgs_arr))/(np.max(fgs_arr)-np.min(fgs_arr))\n    \n    initial_breakpoint=850\n    buffer_size=80 \n    smooth_window=200\n    window_size=300\n    airsbp = optimize_breakpoint(airs_arr,initial_breakpoint,window_size=window_size,buffer_size=buffer_size,smooth_window=smooth_window)\n    fgsbp = optimize_breakpoint(fgs_arr,initial_breakpoint,window_size=window_size,buffer_size=buffer_size,smooth_window=250)\n    midpoint1 = len(airs_arr) // 2\n    bp1 = [airsbp, 2 * midpoint1 - airsbp]\n    airs_start   =  bp1[0] - buffer_size\n    airs_end     =  bp1[0] + buffer_size\n    airs_start1  =  bp1[1] - buffer_size\n    airs_end1    =  bp1[1] + buffer_size\n    \n    midpoint2 = len(fgs_arr) // 2\n    bp2 = [fgsbp, 2 * midpoint2 - fgsbp]\n    fgs_start  =    bp2[0] - buffer_size\n    fgs_end    =    bp2[0] + buffer_size\n    fgs_start1 =    bp2[1] - buffer_size\n    fgs_end1   =    bp2[1] + buffer_size\n    airs_vector=  vector( airs_arr,  airs_start ,  airs_end , airs_start1 , airs_end1 )\n    fgs_vector =  vector( fgs_arr, fgs_start  ,   fgs_end , fgs_start1  , fgs_end1 )        \n    \n    f_vector=  np.concatenate((airs_vector , fgs_vector))\n    \n    return f_vector ","metadata":{"execution":{"iopub.status.busy":"2024-10-23T18:40:27.684872Z","iopub.execute_input":"2024-10-23T18:40:27.685687Z","iopub.status.idle":"2024-10-23T18:40:27.699961Z","shell.execute_reply.started":"2024-10-23T18:40:27.685646Z","shell.execute_reply":"2024-10-23T18:40:27.699081Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def planetnumber(filename):\n    return int(filename.split('_')[0])\n\nclass PlanetDataset(Dataset):\n    def __init__(self, mode):\n        self.mode = mode\n        self.adc_info = pd.read_csv(f'/kaggle/input/ariel-data-challenge-2024/{mode}_adc_info.csv')\n        self.files = sorted(os.listdir(f\"/kaggle/input/ariel-data-challenge-2024/{mode}\"), key=planetnumber)\n            \n    def __len__(self):\n        return len(self.files)\n    \n    def __getitem__(self, idx):\n        planet = int(self.files[idx])\n        selected_adc_info = self.adc_info.loc[self.adc_info['planet_id'] == planet]\n        \n        # Load and process AIRS data\n        airs_dark = pd.read_parquet(f'/kaggle/input/ariel-data-challenge-2024/{self.mode}/{planet}/AIRS-CH0_calibration/dark.parquet').values.reshape(32, 356)\n        airs_dead = pd.read_parquet(f'/kaggle/input/ariel-data-challenge-2024/{self.mode}/{planet}/AIRS-CH0_calibration/dead.parquet').values.reshape(32, 356)\n        airs_flat = pd.read_parquet(f'/kaggle/input/ariel-data-challenge-2024/{self.mode}/{planet}/AIRS-CH0_calibration/flat.parquet').values.reshape(32, 356)\n        airs_linear_corr = pd.read_parquet(f'/kaggle/input/ariel-data-challenge-2024/{self.mode}/{planet}/AIRS-CH0_calibration/linear_corr.parquet').values.reshape(6, 32, 356)\n        airs_ch0 = pd.read_parquet(f'/kaggle/input/ariel-data-challenge-2024/{self.mode}/{planet}/AIRS-CH0_signal.parquet').values\n        airs_ch0_gain = selected_adc_info['AIRS-CH0_adc_gain'].values[0]\n        airs_ch0_offset = selected_adc_info['AIRS-CH0_adc_offset'].values[0]\n        airs_ch0 = airs_ch0.reshape(11250, 32, 356)\n        \n        # Load and process FGS data\n        fgs_dark = pd.read_parquet(f'/kaggle/input/ariel-data-challenge-2024/{self.mode}/{planet}/FGS1_calibration/dark.parquet').values.reshape(32, 32)\n        fgs_dead = pd.read_parquet(f'/kaggle/input/ariel-data-challenge-2024/{self.mode}/{planet}/FGS1_calibration/dead.parquet').values.reshape(32, 32)\n        fgs_flat = pd.read_parquet(f'/kaggle/input/ariel-data-challenge-2024/{self.mode}/{planet}/FGS1_calibration/flat.parquet').values.reshape(32, 32)\n        fgs_linear_corr = pd.read_parquet(f'/kaggle/input/ariel-data-challenge-2024/{self.mode}/{planet}/FGS1_calibration/linear_corr.parquet').values.reshape(6, 32, 32)\n        fgs1 = pd.read_parquet(f'/kaggle/input/ariel-data-challenge-2024/{self.mode}/{planet}/FGS1_signal.parquet').values\n        fgs1_gain = selected_adc_info['FGS1_adc_gain'].values[0]\n        fgs1_offset = selected_adc_info['FGS1_adc_offset'].values[0]\n        fgs1 = fgs1.reshape(135000, 32, 32)\n        \n        # Process signals\n        airs_ch0_processed = process_signal(airs_ch0, airs_ch0_gain, airs_ch0_offset, airs_dead, airs_flat, airs_dark, airs_linear_corr)\n        fgs1_processed = process_signal(fgs1, fgs1_gain, fgs1_offset, fgs_dead, fgs_flat, fgs_dark, fgs_linear_corr)\n        \n        # Bin observations\n        airs_frames = bin_obs(airs_ch0_processed, binning=5)\n        fgs_frames = bin_obs(fgs1_processed, binning=60)\n        \n        # Convert to PyTorch tensors and normalize\n        airs_1d = torch.tensor(np.sum(airs_frames, axis=(1,2))).float()\n        fgs_1d = torch.tensor(np.sum(fgs_frames, axis=(1,2))).float()\n        \n        airs_1d = (airs_1d - airs_1d.min()) / (airs_1d.max() - airs_1d.min())\n        fgs_1d = (fgs_1d - fgs_1d.min()) / (fgs_1d.max() - fgs_1d.min())\n        \n        airs_1d = airs_1d\n        fgs_1d = fgs_1d\n    \n\n        airs_frames = torch.tensor(airs_frames).float().unsqueeze(1)\n        fgs_frames = torch.tensor(fgs_frames).float().unsqueeze(1)\n        \n       # --------------------------------------------------------------------------------------------- \n     \n        airs_frames_2 = bin_obs(airs_ch0_processed, binning=2)\n        fgs_frames_2  = bin_obs(fgs1_processed, binning=25)\n        \n        airs_1d_2     = torch.tensor(np.sum(airs_frames_2, axis=(1,2))).float()\n        fgs_1d_2      = torch.tensor(np.sum(fgs_frames_2 , axis=(1,2))).float()\n        \n        airs_1d_2 = np.array(airs_1d_2)\n        fgs_1d_2 = np.array(fgs_1d_2)\n        \n        input_vector = final_vector( airs_1d_2 , fgs_1d_2)\n        \n        dic = {\n            'airs_frames': airs_frames,\n            'fgs_frames': fgs_frames,\n            'airs_1d': airs_1d,\n            'fgs_1d': fgs_1d,\n        }\n        \n        return planet, input_vector , dic","metadata":{"execution":{"iopub.status.busy":"2024-10-23T18:40:28.589262Z","iopub.execute_input":"2024-10-23T18:40:28.589640Z","iopub.status.idle":"2024-10-23T18:40:28.608476Z","shell.execute_reply.started":"2024-10-23T18:40:28.589605Z","shell.execute_reply":"2024-10-23T18:40:28.607490Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def smooth_signal(signal):\n    x = np.arange(len(signal))\n    smoothed = lowess(signal,x,frac=0.2,it=4,delta=0.1 * np.std(signal),return_sorted=False )\n    return smoothed\n","metadata":{"execution":{"iopub.status.busy":"2024-10-23T18:40:35.746267Z","iopub.execute_input":"2024-10-23T18:40:35.746993Z","iopub.status.idle":"2024-10-23T18:40:35.751675Z","shell.execute_reply.started":"2024-10-23T18:40:35.746954Z","shell.execute_reply":"2024-10-23T18:40:35.750781Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"torch.set_printoptions(precision=10)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-23T18:40:32.895661Z","iopub.execute_input":"2024-10-23T18:40:32.896502Z","iopub.status.idle":"2024-10-23T18:40:32.900480Z","shell.execute_reply.started":"2024-10-23T18:40:32.896462Z","shell.execute_reply":"2024-10-23T18:40:32.899587Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"device1 = torch.device('cuda')\ndevice2 = torch.device(\"cpu\")\n\nairs_frames = 1125\nfgs_frames = 1125\n\nmodel1 = Model(airs_frames, fgs_frames).to(device1)\nmodel1 = nn.DataParallel(model1).to(device1)\n\nmodel2 = ResidualModel().to(device2)\n\n\nweights1=\"/kaggle/input/arieldata/main_model_weights_3.pth\"\nweights2= \"/kaggle/input/arieldata/res_model_weights_4.pth\"\n\n\ncheckpoint1 = torch.load(weights1, map_location=device1 , weights_only=True)\nmodel1.load_state_dict(checkpoint1['model_state_dict'])\n\ncheckpoint2 = torch.load(weights2, map_location=device2 , weights_only=True)\nmodel2.load_state_dict(checkpoint2['model_state_dict'])\n\n\ncuda_count = torch.cuda.device_count()\nprint(f\"Number of CUDA devices: {cuda_count}\")","metadata":{"papermill":{"duration":0.869416,"end_time":"2024-10-15T14:04:45.162400","exception":false,"start_time":"2024-10-15T14:04:44.292984","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-23T18:45:47.521653Z","iopub.execute_input":"2024-10-23T18:45:47.522392Z","iopub.status.idle":"2024-10-23T18:45:47.710126Z","shell.execute_reply.started":"2024-10-23T18:45:47.522344Z","shell.execute_reply":"2024-10-23T18:45:47.709116Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Number of CUDA devices: 2\n","output_type":"stream"}]},{"cell_type":"code","source":"dataset = PlanetDataset(mode=\"test\")\nbatchsize=1\nt=1\ndataloader = DataLoader(dataset, batch_size=batchsize, shuffle=False , num_workers=2)\n\nmodel1.eval()\nmodel2.eval()\nfinal = []\ni=0\nvals=[]\n\nwith torch.no_grad():\n    for planet , vec,  dic in dataloader:\n        \n#         print(\"processing\" , planet )\n        vals.append(planet.item())\n        airs_frames = dic['airs_frames'].to(device1)\n        fgs_frames  = dic['fgs_frames'].to(device1)\n        airs_1d     = dic['airs_1d'].to(device1)\n        fgs_1d      = dic['fgs_1d'].to(device1)\n        \n        output         = model1(airs_frames, fgs_frames, airs_1d, fgs_1d)\n       \n    # --------------------------------------------------------------------------------------------- \n        \n        scales = model2(vec.float())  \n        scales = np.array(scales.detach().cpu())\n        out_1 = np.array(output.detach().cpu())\n        \n#         out_arr =     out_1[0] \n        out_arr =     out_1[0] / scales[0]\n#         print(out_arr[:5])\n    \n        \n#         out_arr = savgol_filter(out_arr, 50, 1)\n        out_arr = smooth_signal(out_arr)\n\n#         print(out_arr[:5])\n        \n        replacement_value = 0.0015\n        out_arr = np.where(np.isnan(out_arr) | (out_arr == 0), replacement_value, out_arr)\n\n        outs  = np.abs(out_arr)\n        final.append(outs)\n            \nprint(\"done\")","metadata":{"execution":{"iopub.status.busy":"2024-10-23T18:47:48.854449Z","iopub.execute_input":"2024-10-23T18:47:48.854853Z","iopub.status.idle":"2024-10-23T18:48:06.134869Z","shell.execute_reply.started":"2024-10-23T18:47:48.854809Z","shell.execute_reply":"2024-10-23T18:48:06.133783Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"done\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","output_type":"stream"}]},{"cell_type":"code","source":"print(len(final))\n# print((out[:5]))\n","metadata":{"execution":{"iopub.status.busy":"2024-10-23T18:52:16.120592Z","iopub.execute_input":"2024-10-23T18:52:16.121276Z","iopub.status.idle":"2024-10-23T18:52:16.125786Z","shell.execute_reply.started":"2024-10-23T18:52:16.121236Z","shell.execute_reply":"2024-10-23T18:52:16.124932Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"1\n","output_type":"stream"}]},{"cell_type":"code","source":"# print(out_arr[0] , planet)\n# print(np.array(final).shape)\n# print(\"yes\")","metadata":{"execution":{"iopub.status.busy":"2024-10-23T18:52:16.404866Z","iopub.execute_input":"2024-10-23T18:52:16.405596Z","iopub.status.idle":"2024-10-23T18:52:16.409034Z","shell.execute_reply.started":"2024-10-23T18:52:16.405560Z","shell.execute_reply":"2024-10-23T18:52:16.408176Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"\npredictions = np.array(final)\ncolumns = ['planet_id'] + [f'wl_{i+1}' for i in range(283)] + [f'sigma_{i+1}' for i in range(283)]\nsubmit_df = pd.DataFrame(columns=columns)\n\nuncertainity = 7e-4\nprint(uncertainity)\nrows_list = []  \n\nfor i in range(len(vals)):  \n    row_data = {\n        'planet_id': vals[i]\n    }\n    \n    for j in range(283):\n        row_data[f'wl_{j+1}'] = predictions[i, j]\n    \n    for j in range(283):\n        row_data[f'sigma_{j+1}'] = uncertainity\n    \n    rows_list.append(row_data)\n\nsubmit_df = pd.DataFrame(rows_list)\n\nsubmit_df.to_csv(\"submission.csv\", index=False)\nprint(\"saved\")","metadata":{"papermill":{"duration":0.062016,"end_time":"2024-10-15T14:05:03.751629","exception":false,"start_time":"2024-10-15T14:05:03.689613","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-23T18:52:16.771775Z","iopub.execute_input":"2024-10-23T18:52:16.772344Z","iopub.status.idle":"2024-10-23T18:52:16.836087Z","shell.execute_reply.started":"2024-10-23T18:52:16.772309Z","shell.execute_reply":"2024-10-23T18:52:16.835283Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"0.0005\nsaved\n","output_type":"stream"}]},{"cell_type":"code","source":"# x=pd.read_csv(\"/kaggle/working/submission.csv\")\n# print(x)","metadata":{"papermill":{"duration":0.004522,"end_time":"2024-10-15T14:05:03.761003","exception":false,"start_time":"2024-10-15T14:05:03.756481","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-21T19:26:43.080809Z","iopub.execute_input":"2024-10-21T19:26:43.081111Z","iopub.status.idle":"2024-10-21T19:26:43.085075Z","shell.execute_reply.started":"2024-10-21T19:26:43.081075Z","shell.execute_reply":"2024-10-21T19:26:43.083996Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}