{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a9904da",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-10-23T14:21:00.903728Z",
     "iopub.status.busy": "2024-10-23T14:21:00.903431Z",
     "iopub.status.idle": "2024-10-23T14:21:04.799423Z",
     "shell.execute_reply": "2024-10-23T14:21:04.798634Z"
    },
    "papermill": {
     "duration": 3.903671,
     "end_time": "2024-10-23T14:21:04.801695",
     "exception": false,
     "start_time": "2024-10-23T14:21:00.898024",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdc7e16a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-23T14:21:04.811870Z",
     "iopub.status.busy": "2024-10-23T14:21:04.811451Z",
     "iopub.status.idle": "2024-10-23T14:21:05.353430Z",
     "shell.execute_reply": "2024-10-23T14:21:05.352365Z"
    },
    "papermill": {
     "duration": 0.549821,
     "end_time": "2024-10-23T14:21:05.355965",
     "exception": false,
     "start_time": "2024-10-23T14:21:04.806144",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CNN2D_AIRS(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(CNN2D_AIRS, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.gn1 = nn.GroupNorm(4, out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.gn2 = nn.GroupNorm(4, out_channels)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.pool(F.relu(self.gn1(self.conv1(x))))\n",
    "        # x = self.pool(F.relu(self.gn2(self.conv2(x))))\n",
    "\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "\n",
    "        return x\n",
    "\n",
    "class CNN2D_FGS(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(CNN2D_FGS, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.gn1 = nn.GroupNorm(4, out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.gn2 = nn.GroupNorm(4, out_channels)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.pool(F.relu(self.gn1(self.conv1(x))))\n",
    "        # x = self.pool(F.relu(self.gn2(self.conv2(x))))\n",
    "\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "\n",
    "        return x\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, airs_frames, fgs_frames):\n",
    "        super(Model, self).__init__()\n",
    "        self.cnn_airs = CNN2D_AIRS(1, 16)\n",
    "        self.cnn_fgs = CNN2D_FGS(1, 16)\n",
    "        \n",
    "        self.lstm_airs = nn.LSTM(16 * 8 * 89, 128, batch_first=True)\n",
    "        self.lstm_fgs = nn.LSTM(16 * 8 * 8, 128, batch_first=True)\n",
    "        \n",
    "        self.ln_airs = nn.LayerNorm(128)\n",
    "        self.ln_fgs = nn.LayerNorm(128)\n",
    "        \n",
    "        self.fc_light_curve_airs = nn.Sequential(\n",
    "            nn.Linear(airs_frames, 64),\n",
    "            nn.ReLU(),\n",
    "#             nn.LayerNorm(64),\n",
    "        )\n",
    "        \n",
    "        self.fc_light_curve_fgs = nn.Sequential(\n",
    "            nn.Linear(fgs_frames, 64),\n",
    "            nn.ReLU(),\n",
    "#             nn.LayerNorm(64),\n",
    "        )\n",
    "        \n",
    "        self.fc_combined = nn.Sequential(            \n",
    "            nn.Linear(128 + 128 + 64 + 64, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.Linear(256, 283)\n",
    "        )\n",
    "    def forward(self, airs_ch0, fgs1, light_curve_airs, light_curve_fgs):\n",
    "        batch_size, frames, _, _, _ = airs_ch0.shape\n",
    "        \n",
    "        airs_features = self.cnn_airs(airs_ch0.view(-1, 1, 32, 356))\n",
    "        airs_features = airs_features.view(batch_size, frames, -1)\n",
    "        _, (airs_hidden, _) = self.lstm_airs(airs_features)\n",
    "        # airs_hidden = self.ln_airs(airs_hidden.squeeze(0))\n",
    "        airs_hidden = airs_hidden.squeeze(0)\n",
    "\n",
    "        \n",
    "        fgs_features = self.cnn_fgs(fgs1.view(-1, 1, 32, 32))\n",
    "        fgs_features = fgs_features.view(batch_size, frames, -1)\n",
    "        _, (fgs_hidden, _) = self.lstm_fgs(fgs_features)\n",
    "        # fgs_hidden = self.ln_fgs(fgs_hidden.squeeze(0))\n",
    "        fgs_hidden = fgs_hidden.squeeze(0)\n",
    "\n",
    "        \n",
    "        light_curve_airs_features = self.fc_light_curve_airs(light_curve_airs)\n",
    "        light_curve_fgs_features = self.fc_light_curve_fgs(light_curve_fgs)\n",
    "        \n",
    "        combined_features = torch.cat((airs_hidden, fgs_hidden, light_curve_airs_features, light_curve_fgs_features), dim=1)\n",
    "        \n",
    "        output = self.fc_combined(combined_features)\n",
    "        return output\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "airs_frames = 1125\n",
    "fgs_frames = 1125\n",
    "model = Model(airs_frames, fgs_frames).to(device) # small batch size model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58477ac0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-23T14:21:05.365645Z",
     "iopub.status.busy": "2024-10-23T14:21:05.365292Z",
     "iopub.status.idle": "2024-10-23T14:21:05.372534Z",
     "shell.execute_reply": "2024-10-23T14:21:05.371706Z"
    },
    "papermill": {
     "duration": 0.014321,
     "end_time": "2024-10-23T14:21:05.374523",
     "exception": false,
     "start_time": "2024-10-23T14:21:05.360202",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class CNN2D_AIRS(nn.Module):\n",
    "#     def __init__(self, in_channels, out_channels):\n",
    "#         super(CNN2D_AIRS, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "#         self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "#         self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "#         self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "#         self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "#     def forward(self, x):\n",
    "# #         x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "# #         x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "\n",
    "#         x = self.pool(F.relu(self.conv1(x)))\n",
    "#         x = self.pool(F.relu(self.conv2(x)))\n",
    "\n",
    "#         return x\n",
    "\n",
    "# class CNN2D_FGS(nn.Module):\n",
    "#     def __init__(self, in_channels, out_channels):\n",
    "#         super(CNN2D_FGS, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "#         self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "#         self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "#         self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "#         self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "#     def forward(self, x):\n",
    "# #         x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "# #         x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "\n",
    "#         x = self.pool(F.relu(self.conv1(x)))\n",
    "#         x = self.pool(F.relu(self.conv2(x)))\n",
    "#         return x\n",
    "\n",
    "# class Model(nn.Module):\n",
    "#     def __init__(self, airs_frames, fgs_frames):\n",
    "#         super(Model, self).__init__()\n",
    "#         self.cnn_airs = CNN2D_AIRS(1, 16)\n",
    "#         self.cnn_fgs = CNN2D_FGS(1, 16)\n",
    "        \n",
    "#         self.lstm_airs = nn.LSTM(16 * 8 * 89, 128, batch_first=True)\n",
    "#         self.lstm_fgs = nn.LSTM(16 * 8 * 8, 128, batch_first=True)\n",
    "        \n",
    "#         self.bn_airs = nn.BatchNorm1d(128)\n",
    "#         self.bn_fgs = nn.BatchNorm1d(128)\n",
    "        \n",
    "#         self.fc_light_curve_airs = nn.Sequential(\n",
    "#             nn.Linear(airs_frames, 64),\n",
    "#             nn.ReLU()\n",
    "# #             nn.BatchNorm1d(64),\n",
    "\n",
    "#         )\n",
    "        \n",
    "#         self.fc_light_curve_fgs = nn.Sequential(\n",
    "#             nn.Linear(fgs_frames, 64),\n",
    "#             nn.ReLU()\n",
    "# #             nn.BatchNorm1d(64),\n",
    "\n",
    "#         )\n",
    "        \n",
    "#         self.fc_combined = nn.Sequential(            \n",
    "#             nn.Linear(128 + 128 + 64 + 64, 256),\n",
    "#             nn.ReLU(),\n",
    "#             nn.BatchNorm1d(256),\n",
    "# #             nn.Dropout(0.15),\n",
    "#             nn.Linear(256, 256),\n",
    "#             nn.ReLU(),\n",
    "#             nn.BatchNorm1d(256),\n",
    "# #             nn.Dropout(0.15),\n",
    "#             nn.Linear(256, 283)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, airs_ch0, fgs1, light_curve_airs, light_curve_fgs):\n",
    "#         batch_size, frames, _, _, _ = airs_ch0.shape\n",
    "        \n",
    "#         airs_features = self.cnn_airs(airs_ch0.view(-1, 1, 32, 356))\n",
    "#         airs_features = airs_features.view(batch_size, frames, -1)\n",
    "#         _, (airs_hidden, _) = self.lstm_airs(airs_features)\n",
    "# #         airs_hidden = self.bn_airs(airs_hidden.squeeze(0))\n",
    "#         airs_hidden = airs_hidden.squeeze(0)\n",
    "        \n",
    "        \n",
    "#         fgs_features = self.cnn_fgs(fgs1.view(-1, 1, 32, 32))\n",
    "#         fgs_features = fgs_features.view(batch_size, frames, -1)\n",
    "#         _, (fgs_hidden, _) = self.lstm_fgs(fgs_features)\n",
    "# #         fgs_hidden = self.bn_fgs(fgs_hidden.squeeze(0))\n",
    "#         fgs_hidden = fgs_hidden.squeeze(0)\n",
    "        \n",
    "#         light_curve_airs_features = self.fc_light_curve_airs(light_curve_airs)\n",
    "#         light_curve_fgs_features = self.fc_light_curve_fgs(light_curve_fgs)\n",
    "        \n",
    "#         combined_features = torch.cat((airs_hidden, fgs_hidden, light_curve_airs_features, light_curve_fgs_features), dim=1)\n",
    "        \n",
    "#         output = self.fc_combined(combined_features)\n",
    "#         return output\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# airs_frames = 1125\n",
    "# fgs_frames = 1125\n",
    "# model = Model(airs_frames, fgs_frames).to(device) # for larger batch size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5959b919",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-23T14:21:05.383371Z",
     "iopub.status.busy": "2024-10-23T14:21:05.383090Z",
     "iopub.status.idle": "2024-10-23T14:21:05.387932Z",
     "shell.execute_reply": "2024-10-23T14:21:05.387083Z"
    },
    "papermill": {
     "duration": 0.011641,
     "end_time": "2024-10-23T14:21:05.390031",
     "exception": false,
     "start_time": "2024-10-23T14:21:05.378390",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6877947\n"
     ]
    }
   ],
   "source": [
    "c=0\n",
    "for x in model.parameters():\n",
    "    c+=x.numel()\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "688747f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-23T14:21:05.399140Z",
     "iopub.status.busy": "2024-10-23T14:21:05.398857Z",
     "iopub.status.idle": "2024-10-23T14:21:05.416251Z",
     "shell.execute_reply": "2024-10-23T14:21:05.415245Z"
    },
    "papermill": {
     "duration": 0.025723,
     "end_time": "2024-10-23T14:21:05.419667",
     "exception": false,
     "start_time": "2024-10-23T14:21:05.393944",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def planetnumber(filename):\n",
    "    return int(filename.split('_')[0])\n",
    "\n",
    "class ARIEL(Dataset):\n",
    "    def __init__(self, airs_dir1, airs_dir2, airs_dir3, airs_dir4, fgs_dir , start , end):\n",
    "        self.airs_dir1 = airs_dir1\n",
    "        self.airs_dir2 = airs_dir2\n",
    "        self.airs_dir3 = airs_dir3\n",
    "        self.airs_dir4 = airs_dir4\n",
    "        self.airs_full = os.listdir(self.airs_dir1) + os.listdir(self.airs_dir2) + os.listdir(self.airs_dir3) + os.listdir(self.airs_dir4) \n",
    "        \n",
    "        self.fgs_dir = fgs_dir\n",
    "        \n",
    "        self.airs_list = sorted(self.airs_full, key=planetnumber)[start:end]\n",
    "        self.fgs_list = sorted(os.listdir(self.fgs_dir), key=planetnumber)[start:end]\n",
    "        \n",
    "        self.labels = pd.read_csv(\"/kaggle/input/ariel-data-challenge-2024/train_labels.csv\")\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        planet= self.airs_list[index]\n",
    "        \n",
    "        if planet in os.listdir(self.airs_dir1):\n",
    "            airs_file = os.path.join(self.airs_dir1, planet )\n",
    "        \n",
    "        elif planet in os.listdir(self.airs_dir2):\n",
    "            airs_file = os.path.join(self.airs_dir2, planet )\n",
    "            \n",
    "        elif planet in os.listdir(self.airs_dir3):\n",
    "            airs_file = os.path.join(self.airs_dir3, planet )\n",
    "            \n",
    "        elif planet in os.listdir(self.airs_dir4):\n",
    "            airs_file = os.path.join(self.airs_dir4, planet )\n",
    "\n",
    "        \n",
    "                    \n",
    "        planet_num = planetnumber(planet)\n",
    "        fgs_file = f\"{self.fgs_dir}/{planet_num}_fgs.npy\" \n",
    "        \n",
    "        airs_arr_frames = np.load(airs_file)\n",
    "        fgs_arr_frames = np.load(fgs_file)\n",
    "        \n",
    "        airs_arr_frames = airs_arr_frames.reshape(1125, 32, 356)\n",
    "        fgs_arr_frames = fgs_arr_frames.reshape(1125, 32, 32)\n",
    "        \n",
    "        airs_1d = np.sum(airs_arr_frames, axis=(1, 2))\n",
    "        fgs_1d = np.sum(fgs_arr_frames, axis=(1, 2))\n",
    "        \n",
    "        airs_1d = (airs_1d-np.min(airs_1d))/(np.max(airs_1d)-np.min(airs_1d))\n",
    "        fgs_1d  = (fgs_1d-np.min(fgs_1d))/(np.max(fgs_1d)-np.min(fgs_1d))\n",
    "\n",
    "        \n",
    "        airs_arr_frames = torch.from_numpy(airs_arr_frames).float().unsqueeze(1)  # Add channel dimension\n",
    "        fgs_arr_frames = torch.from_numpy(fgs_arr_frames).float().unsqueeze(1)  # Add channel dimension\n",
    "        \n",
    "        airs_1d = torch.from_numpy(airs_1d).float()\n",
    "        fgs_1d = torch.from_numpy(fgs_1d).float()\n",
    "        \n",
    "        filtered_data = self.labels[self.labels[\"planet_id\"] == planet_num].iloc[0, 1:].values\n",
    "        output = torch.tensor(filtered_data).float()\n",
    "        \n",
    "        # return  [planet , airs_file , fgs_file]\n",
    "        return {\n",
    "            'airs_frames': airs_arr_frames,\n",
    "            'fgs_frames': fgs_arr_frames,\n",
    "            'airs_1d': airs_1d,\n",
    "            'fgs_1d': fgs_1d,\n",
    "            'label': output\n",
    "        }\n",
    "     \n",
    "    def __len__(self):\n",
    "        return len(self.airs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c73031e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-23T14:21:05.430722Z",
     "iopub.status.busy": "2024-10-23T14:21:05.430423Z",
     "iopub.status.idle": "2024-10-23T14:21:07.000795Z",
     "shell.execute_reply": "2024-10-23T14:21:06.999890Z"
    },
    "papermill": {
     "duration": 1.579164,
     "end_time": "2024-10-23T14:21:07.003852",
     "exception": false,
     "start_time": "2024-10-23T14:21:05.424688",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Training batches: 127, Validation batches: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "airs_frames = 1125\n",
    "fgs_frames = 1125\n",
    "\n",
    "model = Model(airs_frames, fgs_frames).to(device)\n",
    "model = nn.DataParallel(model)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2 , weight_decay=1e-4) \n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.50, patience=3 , min_lr=1e-8)\n",
    "\n",
    "\n",
    "\n",
    "weights = None  \n",
    "# weights=\"/kaggle/input/arieldata/epoch450.pth\"\n",
    "\n",
    "\n",
    "if weights:\n",
    "    checkpoint = torch.load(weights, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    print(f\"Resuming from epoch {start_epoch}\")\n",
    "    print(f\"Resuming learning rate: {optimizer.param_groups[0]['lr']}\")\n",
    "else:\n",
    "    start_epoch = 0\n",
    "\n",
    "train_batchsize = 5\n",
    "val_batchsize   = 2\n",
    "\n",
    "part1=\"/kaggle/input/arieldata/airs-p1\"\n",
    "part2=\"/kaggle/input/arieldata/airs-p2\"\n",
    "part3=\"/kaggle/input/arieldata/airs-p3\"\n",
    "part4=\"/kaggle/input/arieldata/airs-p4\"\n",
    "\n",
    "part5=\"/kaggle/input/arieldata/fgs-p\"\n",
    "\n",
    "train_data = ARIEL(part1 , part2, part3, part4, part5, start=0 , end=635)\n",
    "val_data   = ARIEL(part1 , part2, part3, part4, part5, start=635 , end=665)\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=train_batchsize, shuffle=True, num_workers=8)\n",
    "val_dataloader   = DataLoader(val_data,   batch_size=val_batchsize, shuffle=False, num_workers=8)\n",
    "\n",
    "print(f\"Training batches: {len(train_dataloader)}, Validation batches: {len(val_dataloader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce0d7985",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-23T14:21:07.014754Z",
     "iopub.status.busy": "2024-10-23T14:21:07.014426Z",
     "iopub.status.idle": "2024-10-23T16:06:26.798338Z",
     "shell.execute_reply": "2024-10-23T16:06:26.797188Z"
    },
    "papermill": {
     "duration": 6319.804118,
     "end_time": "2024-10-23T16:06:26.813333",
     "exception": false,
     "start_time": "2024-10-23T14:21:07.009215",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/141, Train loss: 0.05121528625140245, Val loss: 6.763563699981508e-06\n",
      "Model saved at epoch 0, old files clean, 2\n",
      "Epoch 2/141, Train loss: 7.247076293037756e-06, Val loss: 6.788895340529658e-06\n",
      "Epoch 3/141, Train loss: 5.994485772184924e-06, Val loss: 6.287070118560223e-06\n",
      "Removed old checkpoint: /kaggle/working/epoch0.pth\n",
      "Model saved at epoch 2, old files clean, 2\n",
      "Epoch 4/141, Train loss: 8.220314144837604e-06, Val loss: 8.567870948657703e-06\n",
      "Epoch 5/141, Train loss: 1.3761232035746409e-05, Val loss: 8.119530154241753e-06\n",
      " label [0.00086641 0.00087452 0.00087475] , output [0.00203507 0.00344315 0.00632747]\n",
      "Epoch 6/141, Train loss: 1.1088723576760931e-05, Val loss: 6.5191629043207895e-06\n",
      "Epoch 7/141, Train loss: 1.0540873883269163e-05, Val loss: 5.630067759436012e-06\n",
      "Removed old checkpoint: /kaggle/working/epoch2.pth\n",
      "Model saved at epoch 6, old files clean, 2\n",
      "Epoch 8/141, Train loss: 9.060692851261857e-06, Val loss: 9.04861247666607e-06\n",
      "Epoch 9/141, Train loss: 8.00826558044332e-06, Val loss: 6.0597219847598655e-06\n",
      "Epoch 10/141, Train loss: 7.424766105340116e-06, Val loss: 5.691308933819527e-06\n",
      " label [0.00180078 0.00181985 0.00182035] , output [0.00159667 0.00166168 0.00303092]\n",
      "Epoch 11/141, Train loss: 9.238642146162355e-06, Val loss: 5.246302794148505e-06\n",
      "Removed old checkpoint: /kaggle/working/epoch6.pth\n",
      "Model saved at epoch 10, old files clean, 2\n",
      "Epoch 12/141, Train loss: 6.037165008805689e-06, Val loss: 5.8970461092637075e-06\n",
      "Epoch 13/141, Train loss: 7.435860477095606e-06, Val loss: 5.107285020737133e-06\n",
      "Removed old checkpoint: /kaggle/working/epoch10.pth\n",
      "Model saved at epoch 12, old files clean, 2\n",
      "Epoch 14/141, Train loss: 5.664858396963022e-06, Val loss: 4.214980064413491e-06\n",
      "Removed old checkpoint: /kaggle/working/epoch12.pth\n",
      "Model saved at epoch 13, old files clean, 2\n",
      "Epoch 15/141, Train loss: 5.923771992478234e-06, Val loss: 4.438194112784307e-06\n",
      " label [0.00168597 0.00169105 0.00168836] , output [0.00142157 0.00120828 0.00088644]\n",
      "Epoch 16/141, Train loss: 4.870011218469062e-06, Val loss: 4.3554458746560465e-06\n",
      "Epoch 17/141, Train loss: 4.652351293767481e-06, Val loss: 5.183449343348912e-06\n",
      "LR decreased to  0.005\n",
      "Epoch 18/141, Train loss: 4.788080235987685e-06, Val loss: 5.094259161827116e-06\n",
      "Epoch 19/141, Train loss: 3.5923339289083528e-06, Val loss: 3.659719038751064e-06\n",
      "Removed old checkpoint: /kaggle/working/epoch13.pth\n",
      "Model saved at epoch 18, old files clean, 2\n",
      "Epoch 20/141, Train loss: 3.6577141738743246e-06, Val loss: 3.804008561777058e-06\n",
      " label [0.00195231 0.0019688  0.00196707] , output [0.00346774 0.0035919  0.00346164]\n",
      "Epoch 21/141, Train loss: 3.4905309685719535e-06, Val loss: 5.443259204203817e-06\n",
      "Epoch 22/141, Train loss: 3.653485935691298e-06, Val loss: 3.813785751087077e-06\n",
      "Epoch 23/141, Train loss: 3.260073168620886e-06, Val loss: 3.5925893428156997e-06\n",
      "Removed old checkpoint: /kaggle/working/epoch18.pth\n",
      "Model saved at epoch 22, old files clean, 2\n",
      "Epoch 24/141, Train loss: 3.296944293300997e-06, Val loss: 3.5433087361980142e-06\n",
      "Removed old checkpoint: /kaggle/working/epoch22.pth\n",
      "Model saved at epoch 23, old files clean, 2\n",
      "Epoch 25/141, Train loss: 3.0648600265636544e-06, Val loss: 3.807500983536253e-06\n",
      " label [0.00701879 0.00693994 0.00691423] , output [0.00168969 0.0018592  0.00286918]\n",
      "Epoch 26/141, Train loss: 3.0954335921781127e-06, Val loss: 4.270315893487957e-06\n",
      "Epoch 27/141, Train loss: 3.0517927083615284e-06, Val loss: 4.002928244517534e-06\n",
      "LR decreased to  0.0025\n",
      "Epoch 28/141, Train loss: 2.9980884511942135e-06, Val loss: 3.872514442567384e-06\n",
      "Epoch 29/141, Train loss: 3.029646471419241e-06, Val loss: 3.5103298387184624e-06\n",
      "Removed old checkpoint: /kaggle/working/epoch23.pth\n",
      "Model saved at epoch 28, old files clean, 2\n",
      "Epoch 30/141, Train loss: 3.0145328333301723e-06, Val loss: 3.7083014016540497e-06\n",
      " label [0.0010734  0.00107646 0.00107646] , output [0.00239678 0.00236213 0.00229029]\n",
      "Epoch 31/141, Train loss: 2.9757584690194233e-06, Val loss: 3.5330418199919222e-06\n",
      "Epoch 32/141, Train loss: 2.9711952599542185e-06, Val loss: 3.5011328350265105e-06\n",
      "Removed old checkpoint: /kaggle/working/epoch28.pth\n",
      "Model saved at epoch 31, old files clean, 2\n",
      "Epoch 33/141, Train loss: 2.9695360370359285e-06, Val loss: 3.872157590952459e-06\n",
      "Epoch 34/141, Train loss: 3.0083527266430514e-06, Val loss: 3.500783750117383e-06\n",
      "Removed old checkpoint: /kaggle/working/epoch31.pth\n",
      "Model saved at epoch 33, old files clean, 2\n",
      "Epoch 35/141, Train loss: 3.02637184044516e-06, Val loss: 3.513178599708529e-06\n",
      " label [0.00211857 0.00223247 0.0022055 ] , output [0.00246816 0.00252085 0.00256977]\n",
      "LR decreased to  0.00125\n",
      "Epoch 36/141, Train loss: 3.0670850210934824e-06, Val loss: 3.511506082531923e-06\n",
      "Epoch 37/141, Train loss: 2.9627537951741907e-06, Val loss: 3.499931650215634e-06\n",
      "Removed old checkpoint: /kaggle/working/epoch33.pth\n",
      "Model saved at epoch 36, old files clean, 2\n",
      "Epoch 38/141, Train loss: 3.0111390431084432e-06, Val loss: 3.5007870318774318e-06\n",
      "Epoch 39/141, Train loss: 2.9900099179922394e-06, Val loss: 3.559449239295039e-06\n",
      "Epoch 40/141, Train loss: 2.9771558695838894e-06, Val loss: 3.502200326011007e-06\n",
      " label [0.00263387 0.00263412 0.00263059] , output [0.00274047 0.00273354 0.00268569]\n",
      "LR decreased to  0.000625\n",
      "Epoch 41/141, Train loss: 2.9945168569962175e-06, Val loss: 3.5315670174895786e-06\n",
      "Epoch 42/141, Train loss: 2.9619942892182504e-06, Val loss: 3.5008653242130095e-06\n",
      "Epoch 43/141, Train loss: 2.9712057378859516e-06, Val loss: 3.6431699603175124e-06\n",
      "Epoch 44/141, Train loss: 2.983092162091316e-06, Val loss: 3.574730087999948e-06\n",
      "Learning rate manually set to 5e-7 at epoch 44\n",
      "LR decreased to  2.5e-07\n",
      "Epoch 45/141, Train loss: 2.969734763866196e-06, Val loss: 3.5043852127121984e-06\n",
      " label [0.0047604  0.00476392 0.00476293] , output [0.00251498 0.0025484  0.00254828]\n",
      "Epoch 46/141, Train loss: 2.9387097245446744e-06, Val loss: 3.504454171358399e-06\n",
      "Epoch 47/141, Train loss: 2.9387227439255236e-06, Val loss: 3.5044989696568034e-06\n",
      "Epoch 48/141, Train loss: 2.938660500381871e-06, Val loss: 3.5045162462665756e-06\n",
      "LR decreased to  1.25e-07\n",
      "Epoch 49/141, Train loss: 2.938648296950255e-06, Val loss: 3.5046263557584704e-06\n",
      "Epoch 50/141, Train loss: 2.9386018493870487e-06, Val loss: 3.5046421563341332e-06\n",
      " label [0.00093192 0.00098873 0.00098962] , output [0.00251244 0.00254613 0.00254622]\n",
      "Epoch 51/141, Train loss: 2.9385813917986695e-06, Val loss: 3.5046740720190426e-06\n",
      "Epoch 52/141, Train loss: 2.9385870107930035e-06, Val loss: 3.5047200735031464e-06\n",
      "Early stopping triggered at epoch 51\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "epochs = 141\n",
    "total = start_epoch + epochs\n",
    "print(\"training started\")\n",
    "best_val_loss = float('inf')\n",
    "patience = 15\n",
    "no_improve = 0\n",
    "count = 0\n",
    "\n",
    "def cleanup_old_checkpoints(directory):\n",
    "    \"\"\"Remove all .pth files in the specified directory\"\"\"\n",
    "    checkpoint_files = glob.glob(os.path.join(directory, \"*.pth\"))\n",
    "    for f in checkpoint_files:\n",
    "        try:\n",
    "            os.remove(f)\n",
    "            print(f\"Removed old checkpoint: {f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error removing {f}: {e}\")\n",
    "\n",
    "for epoch in range(start_epoch, total):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "    \n",
    "    for batch in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        airs_frames = batch['airs_frames'].to(device)\n",
    "        fgs_frames = batch['fgs_frames'].to(device)\n",
    "        airs_1d = batch['airs_1d'].to(device)\n",
    "        fgs_1d = batch['fgs_1d'].to(device)\n",
    "        label = batch['label'].to(device)\n",
    "        out = model(airs_frames, fgs_frames, airs_1d, fgs_1d)\n",
    "        \n",
    "        loss = criterion(out, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    train_loss /= len(train_dataloader)\n",
    "    \n",
    "    if epoch%5==0 and epoch>0:\n",
    "        print(f\" label {(label[0][:3].cpu().detach().numpy())} , output {(out[0][:3].cpu().detach().numpy())}\")\n",
    "        \n",
    "    if no_improve==7 and count==0 and optimizer.param_groups[0]['lr'] > 1e-6:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = 5e-7\n",
    "            print(f\"Learning rate manually set to 5e-7 at epoch {epoch}\")\n",
    "        count+=1\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            airs_frames = batch['airs_frames'].to(device)\n",
    "            fgs_frames = batch['fgs_frames'].to(device)\n",
    "            airs_1d = batch['airs_1d'].to(device)\n",
    "            fgs_1d = batch['fgs_1d'].to(device)\n",
    "            label = batch['label'].to(device)\n",
    "            out = model(airs_frames, fgs_frames, airs_1d, fgs_1d)\n",
    "            loss = criterion(out, label)\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    val_loss /= len(val_dataloader)\n",
    "    prev = optimizer.param_groups[0]['lr']\n",
    "    scheduler.step(val_loss)\n",
    "    nex = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    if prev!=nex:\n",
    "        print(\"LR decreased to \", nex)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{total}, Train loss: {train_loss}, Val loss: {val_loss}\")\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        no_improve = 0\n",
    "        \n",
    "        cleanup_old_checkpoints(\"/kaggle/working\")\n",
    "        \n",
    "        model_filename = f\"epoch{epoch}.pth\"\n",
    "        model_path = os.path.join(\"/kaggle/working\", model_filename)\n",
    "        \n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'loss': train_loss,\n",
    "        }\n",
    "        torch.save(checkpoint, model_path)        \n",
    "        print(f\"Model saved at epoch {epoch}, old files clean, {len(os.listdir('/kaggle/working'))}\")\n",
    "        \n",
    "    else:\n",
    "        no_improve += 1\n",
    "        if no_improve == patience:\n",
    "            print(\"Early stopping triggered at epoch\", epoch)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5752f5c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-23T16:06:26.834915Z",
     "iopub.status.busy": "2024-10-23T16:06:26.834552Z",
     "iopub.status.idle": "2024-10-23T16:06:26.990553Z",
     "shell.execute_reply": "2024-10-23T16:06:26.989620Z"
    },
    "papermill": {
     "duration": 0.170384,
     "end_time": "2024-10-23T16:06:26.992879",
     "exception": false,
     "start_time": "2024-10-23T16:06:26.822495",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at epoch 51\n"
     ]
    }
   ],
   "source": [
    "model_filename = \"lastrun.pth\"\n",
    "model_path = os.path.join(\"/kaggle/working\", model_filename)\n",
    "checkpoint = {\n",
    "    'epoch': epoch,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'scheduler_state_dict': scheduler.state_dict(),\n",
    "    'loss': train_loss,\n",
    "}\n",
    "torch.save(checkpoint, model_path)        \n",
    "print(f\"Model saved at epoch {epoch}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fd4592f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-23T16:06:27.012737Z",
     "iopub.status.busy": "2024-10-23T16:06:27.012403Z",
     "iopub.status.idle": "2024-10-23T16:06:27.016568Z",
     "shell.execute_reply": "2024-10-23T16:06:27.015787Z"
    },
    "papermill": {
     "duration": 0.016117,
     "end_time": "2024-10-23T16:06:27.018397",
     "exception": false,
     "start_time": "2024-10-23T16:06:27.002280",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model.eval()  \n",
    "# model_filename = f\"epoch{epoch}-loss{epoch_loss:.8f}.pth\"\n",
    "# model_path = os.path.join(\"/kaggle/working\", model_filename)\n",
    "\n",
    "# checkpoint = {\n",
    "#     'epoch': epoch,\n",
    "#     'model_state_dict': model.state_dict(),\n",
    "#     'optimizer_state_dict': optimizer.state_dict(),\n",
    "#     'scheduler_state_dict': scheduler.state_dict(),\n",
    "#     'loss': epoch_loss,\n",
    "# }\n",
    "# torch.save(checkpoint, model_path)        \n",
    "# print(f\"Model saved at epoch {epoch}\")\n",
    "# print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73b02307",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-23T16:06:27.038376Z",
     "iopub.status.busy": "2024-10-23T16:06:27.038064Z",
     "iopub.status.idle": "2024-10-23T16:06:27.042552Z",
     "shell.execute_reply": "2024-10-23T16:06:27.041643Z"
    },
    "papermill": {
     "duration": 0.017147,
     "end_time": "2024-10-23T16:06:27.044719",
     "exception": false,
     "start_time": "2024-10-23T16:06:27.027572",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate final to 1.25e-07 \n"
     ]
    }
   ],
   "source": [
    "print(f\"Learning rate final to {optimizer.param_groups[0]['lr']} \")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 9188054,
     "sourceId": 70367,
     "sourceType": "competition"
    },
    {
     "datasetId": 5618537,
     "isSourceIdPinned": true,
     "sourceId": 9658530,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5912377,
     "sourceId": 9695019,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6331.302256,
   "end_time": "2024-10-23T16:06:29.497578",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-10-23T14:20:58.195322",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
